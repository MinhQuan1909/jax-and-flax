{"cells":[{"cell_type":"code","execution_count":null,"id":"63213bdb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63213bdb","executionInfo":{"status":"ok","timestamp":1717435580661,"user_tz":-420,"elapsed":7461,"user":{"displayName":"Minh Quân","userId":"09257840259269566055"}},"outputId":"834465ba-1783-48fd-ed39-785e2b6f2e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":1,"id":"7175d901","metadata":{"id":"7175d901","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717576252685,"user_tz":-420,"elapsed":90350,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}},"outputId":"0ea3da92-0858-40eb-8377-fb32e965184e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q datasets sacrebleu accelerate>=0.20.1"]},{"cell_type":"code","execution_count":2,"id":"c0bb7005","metadata":{"id":"c0bb7005","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717576289350,"user_tz":-420,"elapsed":36675,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}},"outputId":"24e48a93-fac7-47f6-8928-8dd052146aff"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["import os\n","import numpy as np\n","\n","import sacrebleu\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","from datasets import load_dataset, load_metric\n","from transformers import *"]},{"cell_type":"code","execution_count":3,"id":"b5c1d3f9","metadata":{"id":"b5c1d3f9","executionInfo":{"status":"ok","timestamp":1717576289351,"user_tz":-420,"elapsed":5,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"s1ymlGNhKG24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717576482309,"user_tz":-420,"elapsed":9714,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}},"outputId":"06639036-781a-4194-efa3-2c56ce401adb"},"id":"s1ymlGNhKG24","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"CARf8Jgt_t_6","metadata":{"id":"CARf8Jgt_t_6"},"source":["### 1.Prepare Data"]},{"cell_type":"code","execution_count":6,"id":"877686dc","metadata":{"id":"877686dc","tags":[],"executionInfo":{"status":"ok","timestamp":1717576492416,"user_tz":-420,"elapsed":566,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["class NMTDataset(Dataset):\n","    def __init__(self, cfg, file_path):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.data = pd.read_csv(file_path)\n","        self.src_texts, self.tgt_texts = self.get_src_tgt_texts()\n","        self.src_input_ids, self.src_attention_mask = self.texts_to_sequences(self.src_texts)\n","        self.tgt_input_ids, self.tgt_attention_mask, self.labels = self.texts_to_sequences(\n","            self.tgt_texts,\n","            is_src=False\n","        )\n","\n","    def get_src_tgt_texts(self):\n","        src_texts = self.data[\"en\"].tolist()\n","        tgt_texts = self.data[\"hu\"].tolist()\n","        return src_texts, tgt_texts\n","\n","    def texts_to_sequences(self, texts, is_src=True):\n","        if is_src:\n","            src_inputs = self.cfg.src_tokenizer(\n","                texts,\n","                padding='max_length',\n","                truncation=True,\n","                max_length=self.cfg.src_max_len,\n","                return_tensors='pt'\n","            )\n","            return (\n","                src_inputs.input_ids,\n","                src_inputs.attention_mask\n","            )\n","        else:\n","            if self.cfg.add_special_tokens:\n","                texts = [\n","                    ' '.join([\n","                        self.cfg.tgt_tokenizer.bos_token,\n","                        text,\n","                        self.cfg.tgt_tokenizer.eos_token\n","                        ])\n","                    for text in texts\n","                ]\n","            tgt_inputs = self.cfg.tgt_tokenizer(\n","                texts,\n","                padding='max_length',\n","                truncation=True,\n","                max_length=self.cfg.tgt_max_len,\n","                return_tensors='pt'\n","            )\n","\n","            # Sửa đoạn này: Đảm bảo rằng labels được tạo đúng\n","            labels = tgt_inputs.input_ids.numpy().tolist()\n","            labels = [\n","                [\n","                    -100 if token_id == self.cfg.tgt_tokenizer.pad_token_id else token_id\n","                    for token_id in label\n","                ]\n","                for label in labels\n","            ]\n","            labels = torch.LongTensor(labels)\n","            return (\n","                tgt_inputs.input_ids,\n","                tgt_inputs.attention_mask,\n","                labels\n","            )\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.src_input_ids[idx],\n","            \"attention_mask\": self.src_attention_mask[idx],\n","            \"decoder_input_ids\": self.tgt_input_ids[idx],\n","            \"decoder_attention_mask\": self.tgt_attention_mask[idx],\n","            \"labels\": self.labels[idx]\n","        }\n","\n","    def __len__(self):\n","        return np.shape(self.src_input_ids)[0]"]},{"cell_type":"markdown","id":"ZK0pjhljKWNp","metadata":{"id":"ZK0pjhljKWNp"},"source":["### 2.Load Tokenizer and Model"]},{"cell_type":"code","execution_count":7,"id":"0b17c136","metadata":{"id":"0b17c136","tags":[],"executionInfo":{"status":"ok","timestamp":1717576492852,"user_tz":-420,"elapsed":2,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","    return preds, labels\n","\n","def load_tokenizer(model_name_or_path):\n","        return AutoTokenizer.from_pretrained(model_name_or_path)"]},{"cell_type":"code","source":["class SaveBestModelCallback(TrainerCallback):\n","    def __init__(self, output_dir):\n","        self.output_dir = output_dir\n","        self.best_bleu_score = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        metrics = kwargs[\"metrics\"]\n","        model = kwargs[\"model\"]\n","        tokenizer = kwargs[\"tokenizer\"]\n","\n","        if \"eval_bleu_score\" in metrics:\n","            bleu_score = metrics[\"eval_bleu_score\"]\n","            if bleu_score > self.best_bleu_score:\n","                self.best_bleu_score = bleu_score\n","                print(f\"New best BLEU score: {bleu_score}. Saving model.\")\n","                model.save_pretrained(self.output_dir)\n","        return control"],"metadata":{"id":"f9sMYjQzChlX","executionInfo":{"status":"ok","timestamp":1717576493646,"user_tz":-420,"elapsed":1,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"id":"f9sMYjQzChlX","execution_count":8,"outputs":[]},{"cell_type":"code","source":["# class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n","#     def __init__(self, tokenizer, model, decoder_start_token_id):\n","#         super().__init__(tokenizer, model)\n","#         self.tokenizer = tokenizer\n","#         self.model = model\n","#         self.decoder_start_token_id = decoder_start_token_id  # Lưu trữ giá trị decoder_start_token_id\n","\n","#     def __call__(self, features):\n","#         batch = super().__call__(features)\n","\n","#         if \"labels\" in batch:\n","#             labels = batch[\"labels\"]\n","#             decoder_input_ids = self._shift_tokens_right(labels, self.tokenizer.pad_token_id, self.decoder_start_token_id)\n","#             batch[\"decoder_input_ids\"] = decoder_input_ids\n","\n","#         return batch\n","\n","#     def _shift_tokens_right(self, input_ids, pad_token_id, decoder_start_token_id):\n","#         shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","#         shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n","\n","#         if decoder_start_token_id is None:\n","#             raise ValueError(\"decoder_start_token_id has to be defined.\")\n","\n","#         shifted_input_ids[..., 0] = decoder_start_token_id\n","\n","#         if pad_token_id is None:\n","#             raise ValueError(\"pad_token_id has to be defined.\")\n","\n","#         shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n","\n","#         return shifted_input_ids"],"metadata":{"id":"GhVAIZ0Fb2wc","executionInfo":{"status":"ok","timestamp":1717576495020,"user_tz":-420,"elapsed":1,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"id":"GhVAIZ0Fb2wc","execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"id":"97b73613","metadata":{"id":"97b73613","tags":[],"executionInfo":{"status":"ok","timestamp":1717576495020,"user_tz":-420,"elapsed":1,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["class Manager():\n","    def __init__(self, cfg, file_path, fold_index=1, best_bleu_score = 0, is_train=True):\n","        self.cfg = cfg\n","        self.fold_index = fold_index\n","        self.best_bleu_score = best_bleu_score\n","\n","        print(\"Loading Tokenizer...\")\n","        self.get_tokenizer()\n","\n","        print(\"Loading Model...\")\n","        self.get_model()\n","\n","        print(\"Loading Metric...\")\n","        self.bleu_metric = load_metric(\"sacrebleu\")\n","\n","        print(\"Check Save Model Path\")\n","        if not os.path.exists(self.cfg.ckpt_dir):\n","            os.mkdir(self.cfg.ckpt_dir)\n","\n","        if is_train:\n","            print(\"Loading Dataset...\")\n","            file_train_path = file_path + f'fold_{fold_index}_train.csv'\n","            file_valid_path = file_path + f'fold_{fold_index}_valid.csv'\n","            self.train_dataset = NMTDataset(self.cfg, file_train_path)\n","            self.valid_dataset = NMTDataset(self.cfg, file_valid_path)\n","\n","        print(\"Setting finished.\")\n","\n","    def get_tokenizer(self):\n","        if self.cfg.load_model_from_path:\n","            self.cfg.src_tokenizer = load_tokenizer(self.cfg.ckpt_dir)\n","            self.cfg.tgt_tokenizer = load_tokenizer(self.cfg.ckpt_dir)\n","        else:\n","            self.cfg.src_tokenizer = load_tokenizer(self.cfg.src_model_name)\n","            self.cfg.tgt_tokenizer = load_tokenizer(self.cfg.tgt_model_name)\n","            if \"bert\" in self.cfg.tgt_model_name.split('-'):\n","                self.cfg.add_special_tokens = False\n","                self.cfg.bos_token_id = self.cfg.tgt_tokenizer.cls_token_id\n","                self.cfg.eos_token_id = self.cfg.tgt_tokenizer.sep_token_id\n","                self.cfg.pad_token_id = self.cfg.tgt_tokenizer.pad_token_id\n","            else:\n","                self.cfg.add_special_tokens = True\n","                self.cfg.tgt_tokenizer.add_special_tokens(\n","                    {\n","                        \"bos_token\": \"[BOS]\",\n","                        \"eos_token\": \"[EOS]\",\n","                        \"pad_token\": \"[PAD]\"\n","                    }\n","                )\n","                self.cfg.bos_token_id = self.cfg.tgt_tokenizer.bos_token_id\n","                self.cfg.eos_token_id = self.cfg.tgt_tokenizer.eos_token_id\n","                self.cfg.pad_token_id = self.cfg.tgt_tokenizer.pad_token_id\n","        self.cfg.src_tokenizer.save_pretrained(\n","                os.path.join(self.cfg.ckpt_dir, f\"{self.cfg.src_lang}_tokenizer_{cfg.src_model_name}\")\n","            )\n","\n","        self.cfg.tgt_tokenizer.save_pretrained(\n","                os.path.join(self.cfg.ckpt_dir, f\"{self.cfg.tgt_lang}_tokenizer_{cfg.tgt_model_name}\")\n","            )\n","\n","    def get_model(self):\n","        if self.cfg.load_model_from_path:\n","            save_model_path = os.path.join(self.cfg.ckpt_dir, self.cfg.ckpt_name)\n","            self.model = EncoderDecoderModel.from_pretrained(save_model_path)\n","        else:\n","            self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n","                self.cfg.src_model_name,\n","                self.cfg.tgt_model_name\n","            )\n","            self.model.decoder.resize_token_embeddings(len(self.cfg.tgt_tokenizer))\n","            self.model.config.decoder_start_token_id = self.cfg.bos_token_id\n","            self.model.config.eos_token_id = self.cfg.eos_token_id\n","            self.model.config.pad_token_id = self.cfg.pad_token_id\n","            self.model.config.vocab_size = len(self.cfg.tgt_tokenizer)\n","            self.model.config.max_length = self.cfg.max_length_decoder\n","            self.model.config.min_length = self.cfg.min_length_decoder\n","            self.model.config.no_repeat_ngram_size = 3\n","            self.model.config.early_stopping = True\n","            self.model.config.length_penalty = 1.0\n","            self.model.config.num_beams = self.cfg.beam_size\n","\n","    def train(self):\n","        print(\"Training...\")\n","        if self.cfg.use_eval_steps:\n","            training_args = Seq2SeqTrainingArguments(\n","                predict_with_generate=True,\n","                evaluation_strategy=\"steps\",\n","                save_strategy='steps',\n","                save_steps=self.cfg.eval_steps,\n","                eval_steps=self.cfg.eval_steps,\n","                output_dir=self.cfg.ckpt_dir,\n","                per_device_train_batch_size=self.cfg.train_batch_size,\n","                per_device_eval_batch_size=self.cfg.eval_batch_size,\n","                learning_rate=self.cfg.learning_rate,\n","                weight_decay=0.005,\n","                num_train_epochs=self.cfg.num_train_epochs\n","            )\n","        else:\n","            training_args = Seq2SeqTrainingArguments(\n","                predict_with_generate=True,\n","                evaluation_strategy=\"epoch\",\n","                save_strategy='epoch',\n","                output_dir=self.cfg.ckpt_dir,\n","                per_device_train_batch_size=self.cfg.train_batch_size,\n","                per_device_eval_batch_size=self.cfg.eval_batch_size,\n","                learning_rate=self.cfg.learning_rate,\n","                weight_decay=0.005,\n","                num_train_epochs=self.cfg.num_train_epochs\n","            )\n","\n","        data_collator = DataCollatorForSeq2Seq(\n","            tokenizer=self.cfg.tgt_tokenizer,\n","            model=self.model,\n","        )\n","\n","        #####################\n","        # for batch in DataLoader(self.train_dataset, batch_size=1, collate_fn=data_collator):\n","        #     print(batch)\n","        #     break\n","\n","        trainer = Seq2SeqTrainer(\n","            self.model,\n","            training_args,\n","            train_dataset=self.train_dataset,\n","            eval_dataset=self.valid_dataset,\n","            data_collator=data_collator,\n","            #tokenizer=self.cfg.tgt_tokenizer,\n","            compute_metrics=self.compute_metrics,\n","            callbacks=[SaveBestModelCallback(output_dir=self.cfg.ckpt_dir)]\n","        )\n","\n","        trainer.train()\n","\n","    def compute_metrics(self, eval_preds):\n","        preds, labels = eval_preds\n","        if isinstance(preds, tuple):\n","            preds = preds[0]\n","        decoded_preds = self.cfg.tgt_tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","        labels = np.where(labels != -100, labels, self.cfg.tgt_tokenizer.pad_token_id)\n","        decoded_labels = self.cfg.tgt_tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","        result = self.bleu_metric.compute(\n","            predictions=decoded_preds,\n","            references=decoded_labels\n","        )\n","\n","        result = {\"bleu_score\": result[\"score\"]}\n","\n","        prediction_lens = [np.count_nonzero(pred != self.cfg.tgt_tokenizer.pad_token_id) for pred in preds]\n","        result[\"gen_len\"] = np.mean(prediction_lens)\n","        result = {k: round(v, 4) for k, v in result.items()}\n","\n","        # Check if the BLEU score is the best and save the model if it is\n","        if result[\"bleu_score\"] > self.best_bleu_score:\n","            print('Save model with bleu score: ', result[\"bleu_score\"])\n","            self.best_bleu_score = result[\"bleu_score\"]\n","            self.model.save_pretrained(os.path.join(self.cfg.ckpt_dir, f\"best_model_fold_{self.fold_index}\"))\n","\n","        return result"]},{"cell_type":"markdown","id":"NLQcCekFKc67","metadata":{"id":"NLQcCekFKc67"},"source":["##3.Config"]},{"cell_type":"code","execution_count":13,"id":"268232ec","metadata":{"id":"268232ec","tags":[],"executionInfo":{"status":"ok","timestamp":1717576514731,"user_tz":-420,"elapsed":389,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["class BaseConfig:\n","    \"\"\" base Encoder Decoder config \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","\n","class NMTConfig(BaseConfig):\n","    # Data\n","    src_lang = 'en'\n","    tgt_lang = 'hu'\n","    src_max_len = 100\n","    tgt_max_len = 100 #####################################\n","\n","    # Model\n","    src_model_name = \"bert-base-multilingual-cased\"\n","    tgt_model_name = \"bert-base-multilingual-cased\"\n","    # src_model_name = \"FacebookAI/xlm-roberta-base\"\n","    # tgt_model_name = \"FacebookAI/xlm-roberta-base\"\n","\n","    # Training\n","    load_model_from_path = False\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    learning_rate = 3e-5\n","    train_batch_size = 4\n","    eval_batch_size = 4\n","    num_train_epochs = 20\n","    ckpt_dir = '/content/drive/MyDrive/jax and flax/' + src_model_name.split('/')[-1] + '_to_' + tgt_model_name.split('/')[-1]\n","    use_eval_steps = False\n","    eval_steps = 2000\n","\n","    # Inference\n","    max_length_decoder = 75\n","    min_length_decoder = 25  ######################################\n","    beam_size = 3\n","\n","cfg = NMTConfig()"]},{"cell_type":"markdown","id":"Oq5pd77pKf1N","metadata":{"id":"Oq5pd77pKf1N"},"source":["##4.Training"]},{"cell_type":"code","source":["# #xóa folder dư\n","# import shutil\n","# shutil.rmtree(\"/content/bert-base-multilingual-cased_to_bert-base-multilingual-cased\")"],"metadata":{"id":"zpAKHaoKQB_j","executionInfo":{"status":"ok","timestamp":1717576515567,"user_tz":-420,"elapsed":2,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"id":"zpAKHaoKQB_j","execution_count":14,"outputs":[]},{"cell_type":"code","source":["file_path=\"/content/drive/MyDrive/jax and flax/data1000/\"\n","def run_cross_validation(cfg, file_path, k = 1):\n","    best_bleu_score = 0\n","    best_fold_index = 0\n","\n","    for fold_index in range(1, k + 1):\n","        print(f'Running Fold {fold_index}')\n","        manager = Manager(cfg, file_path, fold_index, best_bleu_score, is_train=True)\n","        manager.train()\n","\n","        bleu_score = manager.best_bleu_score\n","\n","        if bleu_score > best_bleu_score:\n","            best_bleu_score = bleu_score\n","            best_fold_index = fold_index\n","\n","    print(f\"Best BLEU score: {best_bleu_score} found in fold {best_fold_index}\")\n","    print(f\"Best model saved at: {os.path.join(cfg.ckpt_dir, f'best_model_fold_{best_fold_index}')}\")"],"metadata":{"id":"2qhJZKP7VevV","executionInfo":{"status":"ok","timestamp":1717576518206,"user_tz":-420,"elapsed":1,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"id":"2qhJZKP7VevV","execution_count":15,"outputs":[]},{"cell_type":"code","source":["run_cross_validation(cfg, file_path, k = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ba6f11e15c1f4ef6a4d3b7f9d1493269","615f830d34f6403c9ee8e1afbf5305db","75000eefc36040eaaa64c0bbff82063f","1f0e6bc0e8ca4009a3a6d637abc2077b","b81eb5187fdb45bdb6825cc879733134","cb2257f7cf5b4451b1c67534ffa14a1e","f8d2ce8b3ef2457b911605266efa19db","fc5e09a996e94951b24e6f5dcf6680b9","e84057dca0354602971def906ebe41af","0b6bb6de5f824615a98133136658961f","839057c168a048988417cffd439d8064","7abe32d24621486ba87d36a5e547927a","da73ec5b0522420099bf54b015203439","8a5077e7a6f64bb58b45f9665ac33cc8","455559d97c354479ba0bc0e4ffb173c4","8e21f2375941428a9842242ece2c346c","2d717966b0bb40939e89f75f6d72b560","5c71ad3512e348d7a193ba053ab689cf","06609e39eec04943b04e53502df25e37","0dcbe87944ac4ef38bd1f4960e2ced13","3462d478b6c04dfc841674f842c05c3c","13b8531d354a4873a2de3d2d8671c994","afa8c58d86854543863e5752e1f33da5","70b15cfb6df24386a75dd0c6851a7f24","7af8439a550244f4b7bd389164b7e048","245b13f9bc79406d8154c34864f9874d","81482252379943629314b423364d0d5e","5e67b523c8ec4e45b1b627484eaf934c","e76da80ecb2b4e948acb307da1e3cc6e","c3ae61216f964ad9ac5bc46a58d178ca","fa6472edc59243cb8be38802bc75f533","6ca9719ac46648fa9d7a877aa229f7be","c9332171eac64d46ac315993df962343","f287d907490743e6a29f422b75073e02","b4f05dd8c3874057a8310956f8d8f545","2ad1353dc75f42bba00755aa0f5d5eb6","f1c4e2c90bee4b67bda6e2e1f0e55e1f","d6d8218933154b0eae846f2f746b487c","d6be7767ed3f4f45a5ef699a558cf960","6bf78d90f9b0413ba89f4d695a81daba","75770492171843418a0699aeecd7efef","69d1b1811cd54ccd95ae08ba88e18811","74f5d03fc95b49669556406d2ab789c9","c04a3dbf16784e95a1765f8941c97bd5","a1add8f686d74f3ea078aebaba14e489","9fbbd16f78bc41a1bfb81a94ab031b7b","9888a0f6df6f46d9b6eb47607a829db6","ebe8b643d7b14171958576832f7c4a9f","9e3aff07e210471e8f367f1739db14b5","a7ab194bdd6348a9b1d60e31d18e2f02","5c1361884b944ce8860dff80ec39e6af","9e3a023bf84c45998986ff1bf788b0c0","b4ce2e016e5749d6a522538b7532f6d6","52ef2bd868274d59b8a0621010686fba","0977361ef1d54d7194b0874dae08e5dd","9478cd4f615b466b93dcd19fae703674","da9e1c858f304908838c60af50158c50","6e59a3038bf4440daa757578b4e6b264","306d8bf6046847e8878d16cf3858382f","720cf32032fd4b358097cbd287278803","bc7a9b627aa641cbab897fda1a079a90","120458580ab74e2b8784875ade240e2e","b3cc8821c7f64fb181d91202fdcfe655","bf10cf6dce264a109b58be433f4da8ce","2560a18aa0274803b73a0012e0744bc7","b2a286e33a684eb3b4dd4f746bb54c3f"]},"id":"W2Gfl_IGWsqy","executionInfo":{"status":"error","timestamp":1717578820547,"user_tz":-420,"elapsed":1758187,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}},"outputId":"2de95d86-9ff8-4004-96b4-59bf40d70847"},"id":"W2Gfl_IGWsqy","execution_count":16,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running Fold 1\n","Loading Tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba6f11e15c1f4ef6a4d3b7f9d1493269","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7abe32d24621486ba87d36a5e547927a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afa8c58d86854543863e5752e1f33da5","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f287d907490743e6a29f422b75073e02","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","tokenizer config file saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/en_tokenizer_bert-base-multilingual-cased/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/en_tokenizer_bert-base-multilingual-cased/special_tokens_map.json\n","tokenizer config file saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/hu_tokenizer_bert-base-multilingual-cased/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/hu_tokenizer_bert-base-multilingual-cased/special_tokens_map.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading Model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1add8f686d74f3ea078aebaba14e489","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","Initializing bert-base-multilingual-cased as a decoder model. Cross attention layers are added to bert-base-multilingual-cased and randomly initialized if bert-base-multilingual-cased's architecture allows for cross attention layers.\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n","Generate config GenerationConfig {\n","  \"pad_token_id\": 0\n","}\n","\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertLMHeadModel: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Generation config file not found, using a generation config created from the model config.\n","Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n","Generate config GenerationConfig {}\n","\n","You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 119547. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n","<ipython-input-10-e40640042658>:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  self.bleu_metric = load_metric(\"sacrebleu\")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/sacrebleu/sacrebleu.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading Metric...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9478cd4f615b466b93dcd19fae703674","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Check Save Model Path\n","Loading Dataset...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Setting finished.\n","Training...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 810\n","  Num Epochs = 20\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4,060\n","  Number of trainable parameters = 384,194,811\n","/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:646: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='610' max='4060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 610/4060 07:17 < 41:20, 1.39 it/s, Epoch 3/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Score</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>4.085449</td>\n","      <td>1.944300</td>\n","      <td>34.877800</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>3.778748</td>\n","      <td>1.709900</td>\n","      <td>37.555600</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23/23 00:27]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 101,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 102,\n","  \"max_length\": 75,\n","  \"min_length\": 25,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 3,\n","  \"pad_token_id\": 0\n","}\n","\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Save model with bleu score:  1.9443\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/generation_config.json\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["New best BLEU score: 1.9443. Saving model.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/model.safetensors\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-203\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-203/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-203/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-203/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-406\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-406/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-406/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-406/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Save model with bleu score:  1.9721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/config.json\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["New best BLEU score: 1.9721. Saving model.\n"]},{"output_type":"stream","name":"stderr","text":["Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/model.safetensors\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2640' max='4060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2640/4060 37:13 < 20:02, 1.18 it/s, Epoch 13/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Score</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>4.085449</td>\n","      <td>1.944300</td>\n","      <td>34.877800</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>3.778748</td>\n","      <td>1.709900</td>\n","      <td>37.555600</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>4.421300</td>\n","      <td>3.689551</td>\n","      <td>1.972100</td>\n","      <td>34.855600</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4.421300</td>\n","      <td>3.707013</td>\n","      <td>1.988700</td>\n","      <td>37.888900</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.000700</td>\n","      <td>3.752311</td>\n","      <td>2.765300</td>\n","      <td>37.155600</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>3.000700</td>\n","      <td>3.975467</td>\n","      <td>2.990500</td>\n","      <td>37.833300</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>3.000700</td>\n","      <td>4.113042</td>\n","      <td>2.692000</td>\n","      <td>39.644400</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.109700</td>\n","      <td>4.333920</td>\n","      <td>2.308300</td>\n","      <td>35.811100</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.109700</td>\n","      <td>4.501572</td>\n","      <td>2.208400</td>\n","      <td>37.588900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.395400</td>\n","      <td>4.639874</td>\n","      <td>2.674300</td>\n","      <td>33.300000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.395400</td>\n","      <td>4.759329</td>\n","      <td>2.220400</td>\n","      <td>38.277800</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.395400</td>\n","      <td>4.911828</td>\n","      <td>2.097800</td>\n","      <td>38.811100</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.859200</td>\n","      <td>5.001357</td>\n","      <td>2.442200</td>\n","      <td>36.388900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-609\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-609/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-609/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-609/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n"]},{"output_type":"stream","name":"stdout","text":["Save model with bleu score:  1.9887\n"]},{"output_type":"stream","name":"stderr","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/generation_config.json\n"]},{"output_type":"stream","name":"stdout","text":["New best BLEU score: 1.9887. Saving model.\n"]},{"output_type":"stream","name":"stderr","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/model.safetensors\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-812\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-812/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-812/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-812/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n"]},{"output_type":"stream","name":"stdout","text":["Save model with bleu score:  2.7653\n"]},{"output_type":"stream","name":"stderr","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/generation_config.json\n"]},{"output_type":"stream","name":"stdout","text":["New best BLEU score: 2.7653. Saving model.\n"]},{"output_type":"stream","name":"stderr","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/model.safetensors\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1015\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1015/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1015/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1015/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n"]},{"output_type":"stream","name":"stdout","text":["Save model with bleu score:  2.9905\n"]},{"output_type":"stream","name":"stderr","text":["Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n"]},{"output_type":"stream","name":"stdout","text":["New best BLEU score: 2.9905. Saving model.\n"]},{"output_type":"stream","name":"stderr","text":["Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/model.safetensors\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1218\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1218/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1218/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1218/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1421\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1421/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1421/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1421/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1624\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1624/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1624/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1624/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1827\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1827/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1827/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-1827/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2030\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2030/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2030/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2030/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2233\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2233/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2233/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2233/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2436\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2436/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2436/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2436/model.safetensors\n","/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 90\n","  Batch size = 4\n","Saving model checkpoint to /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2639\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 75, 'min_length': 25, 'early_stopping': True, 'num_beams': 3, 'no_repeat_ngram_size': 3}\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2639/config.json\n","Configuration saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2639/generation_config.json\n","Model weights saved in /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/checkpoint-2639/model.safetensors\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:595] . unexpected pos 1161708736 vs 1161708628","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/226: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-f7d85233987e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-d2a75de79f76>\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[0;34m(cfg, file_path, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running Fold {fold_index}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_bleu_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_bleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-e40640042658>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2732\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2815\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2816\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   2923\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2925\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 1161708736 vs 1161708628"]}]},{"cell_type":"markdown","id":"jx2jRxqdKjSO","metadata":{"id":"jx2jRxqdKjSO"},"source":["##5.Evaluate"]},{"cell_type":"code","execution_count":17,"id":"724fa565","metadata":{"id":"724fa565","executionInfo":{"status":"ok","timestamp":1717579337202,"user_tz":-420,"elapsed":596,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["def load_model(cfg, checkpoint_name):\n","    # Load Tokenizer\n","    src_tokenizer_save_path = f\"{cfg.ckpt_dir}/{cfg.src_lang}_tokenizer_{cfg.src_model_name}\"\n","    src_tokenizer = AutoTokenizer.from_pretrained(src_tokenizer_save_path)\n","\n","    tgt_tokenizer_save_path = f\"{cfg.ckpt_dir}/{cfg.tgt_lang}_tokenizer_{cfg.tgt_model_name}\"\n","    tgt_tokenizer = AutoTokenizer.from_pretrained(tgt_tokenizer_save_path)\n","\n","    # Load Model\n","    model_save_path = f\"{cfg.ckpt_dir}/{checkpoint_name}\"\n","    model = EncoderDecoderModel.from_pretrained(model_save_path)\n","\n","    # Inference Param\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    return src_tokenizer, tgt_tokenizer, model, device"]},{"cell_type":"code","execution_count":18,"id":"56f46712","metadata":{"id":"56f46712","executionInfo":{"status":"ok","timestamp":1717579337801,"user_tz":-420,"elapsed":1,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["from tqdm import tqdm\n","def inference(\n","    text,\n","    src_tokenizer,\n","    tgt_tokenizer,\n","    model,\n","    device=\"cpu\",\n","    max_length=75,\n","    beam_size=5\n","    ):\n","    inputs = src_tokenizer(\n","        text,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_length,\n","        return_tensors=\"pt\"\n","        )\n","    input_ids = inputs.input_ids.to(device)\n","    attention_mask = inputs.attention_mask.to(device)\n","    model.to(device)\n","\n","    outputs = model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        max_length=max_length,\n","        early_stopping=True,\n","        num_beams=beam_size,\n","        length_penalty=2.0\n","    )\n","\n","    output_str = tgt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    return output_str\n","\n","def inference_bath(\n","    texts,\n","    src_tokenizer,\n","    tgt_tokenizer,\n","    model,\n","    device=\"cpu\",\n","    max_length=75,\n","    beam_size=5,\n","    batch_size=32\n","    ):\n","\n","    pred_texts = []\n","\n","    if len(texts) < batch_size:\n","        batch_size = len(texts)\n","\n","    for x in tqdm(range(0, len(texts), batch_size)):\n","        text = texts[x:x+batch_size]\n","\n","        inputs = src_tokenizer(\n","            text,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","            )\n","\n","        input_ids = inputs.input_ids.to(device)\n","        attention_mask = inputs.attention_mask.to(device)\n","        model.to(device)\n","\n","        outputs = model.generate(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            max_length=max_length,\n","            early_stopping=True,\n","            num_beams=beam_size,\n","            length_penalty=2.0\n","        )\n","\n","        output_str = tgt_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        pred_texts.extend(output_str)\n","        torch.cuda.empty_cache()\n","\n","    return pred_texts"]},{"cell_type":"code","execution_count":19,"id":"3KDuUw1r85Fi","metadata":{"id":"3KDuUw1r85Fi","executionInfo":{"status":"ok","timestamp":1717579339579,"user_tz":-420,"elapsed":2,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["class BaseConfig:\n","    \"\"\" base Encoder Decoder config \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","\n","class NMTConfig(BaseConfig):\n","    # Data\n","    src_lang = 'en'\n","    tgt_lang = 'hu'\n","    src_max_len = 75\n","    tgt_max_len = 75\n","\n","    # Model\n","    src_model_name = \"bert-base-multilingual-cased\"\n","    tgt_model_name = \"bert-base-multilingual-cased\"\n","\n","    # Training\n","    load_model_from_path = False\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    learning_rate = 3e-5\n","    train_batch_size = 16\n","    eval_batch_size = 8\n","    num_train_epochs =15\n","    ckpt_dir = '/content/drive/MyDrive/jax and flax/' + src_model_name + '_to_' + tgt_model_name\n","    use_eval_steps = False\n","    eval_steps = 2000\n","\n","    # Inference\n","    max_length_decoder = 75\n","    min_length_decoder = 25\n","    beam_size = 5\n","\n","cfg = NMTConfig()"]},{"cell_type":"code","execution_count":24,"id":"7f322730","metadata":{"id":"7f322730","executionInfo":{"status":"ok","timestamp":1717579464334,"user_tz":-420,"elapsed":629,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}}},"outputs":[],"source":["data_path = '/content/drive/MyDrive/jax and flax/data1000/test_data.csv'  # Đường dẫn tới tệp test_data.csv\n","test_df = pd.read_csv(data_path)\n","\n","src_texts = test_df['en'].tolist()\n","tgt_texts = test_df['hu'].tolist()"]},{"cell_type":"code","execution_count":25,"id":"7626fb56","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49529,"status":"ok","timestamp":1717579549489,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"},"user_tz":-420},"id":"7626fb56","outputId":"4a68d8fe-2db2-4666-e67c-fb5ee8c4d34b"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading configuration file /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/config.json\n","Model config EncoderDecoderConfig {\n","  \"architectures\": [\n","    \"EncoderDecoderModel\"\n","  ],\n","  \"decoder\": {\n","    \"_name_or_path\": \"bert-base-multilingual-cased\",\n","    \"add_cross_attention\": true,\n","    \"architectures\": [\n","      \"BertForMaskedLM\"\n","    ],\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"bad_words_ids\": null,\n","    \"begin_suppress_tokens\": null,\n","    \"bos_token_id\": null,\n","    \"chunk_size_feed_forward\": 0,\n","    \"classifier_dropout\": null,\n","    \"cross_attention_hidden_size\": null,\n","    \"decoder_start_token_id\": null,\n","    \"directionality\": \"bidi\",\n","    \"diversity_penalty\": 0.0,\n","    \"do_sample\": false,\n","    \"early_stopping\": false,\n","    \"encoder_no_repeat_ngram_size\": 0,\n","    \"eos_token_id\": null,\n","    \"exponential_decay_length_penalty\": null,\n","    \"finetuning_task\": null,\n","    \"forced_bos_token_id\": null,\n","    \"forced_eos_token_id\": null,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.1,\n","    \"hidden_size\": 768,\n","    \"id2label\": {\n","      \"0\": \"LABEL_0\",\n","      \"1\": \"LABEL_1\"\n","    },\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 3072,\n","    \"is_decoder\": true,\n","    \"is_encoder_decoder\": false,\n","    \"label2id\": {\n","      \"LABEL_0\": 0,\n","      \"LABEL_1\": 1\n","    },\n","    \"layer_norm_eps\": 1e-12,\n","    \"length_penalty\": 1.0,\n","    \"max_length\": 20,\n","    \"max_position_embeddings\": 512,\n","    \"min_length\": 0,\n","    \"model_type\": \"bert\",\n","    \"no_repeat_ngram_size\": 0,\n","    \"num_attention_heads\": 12,\n","    \"num_beam_groups\": 1,\n","    \"num_beams\": 1,\n","    \"num_hidden_layers\": 12,\n","    \"num_return_sequences\": 1,\n","    \"output_attentions\": false,\n","    \"output_hidden_states\": false,\n","    \"output_scores\": false,\n","    \"pad_token_id\": 0,\n","    \"pooler_fc_size\": 768,\n","    \"pooler_num_attention_heads\": 12,\n","    \"pooler_num_fc_layers\": 3,\n","    \"pooler_size_per_head\": 128,\n","    \"pooler_type\": \"first_token_transform\",\n","    \"position_embedding_type\": \"absolute\",\n","    \"prefix\": null,\n","    \"problem_type\": null,\n","    \"pruned_heads\": {},\n","    \"remove_invalid_values\": false,\n","    \"repetition_penalty\": 1.0,\n","    \"return_dict\": true,\n","    \"return_dict_in_generate\": false,\n","    \"sep_token_id\": null,\n","    \"suppress_tokens\": null,\n","    \"task_specific_params\": null,\n","    \"temperature\": 1.0,\n","    \"tf_legacy_loss\": false,\n","    \"tie_encoder_decoder\": false,\n","    \"tie_word_embeddings\": true,\n","    \"tokenizer_class\": null,\n","    \"top_k\": 50,\n","    \"top_p\": 1.0,\n","    \"torch_dtype\": null,\n","    \"torchscript\": false,\n","    \"type_vocab_size\": 2,\n","    \"typical_p\": 1.0,\n","    \"use_bfloat16\": false,\n","    \"use_cache\": true,\n","    \"vocab_size\": 119547\n","  },\n","  \"decoder_start_token_id\": 101,\n","  \"early_stopping\": true,\n","  \"encoder\": {\n","    \"_name_or_path\": \"bert-base-multilingual-cased\",\n","    \"add_cross_attention\": false,\n","    \"architectures\": [\n","      \"BertForMaskedLM\"\n","    ],\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"bad_words_ids\": null,\n","    \"begin_suppress_tokens\": null,\n","    \"bos_token_id\": null,\n","    \"chunk_size_feed_forward\": 0,\n","    \"classifier_dropout\": null,\n","    \"cross_attention_hidden_size\": null,\n","    \"decoder_start_token_id\": null,\n","    \"directionality\": \"bidi\",\n","    \"diversity_penalty\": 0.0,\n","    \"do_sample\": false,\n","    \"early_stopping\": false,\n","    \"encoder_no_repeat_ngram_size\": 0,\n","    \"eos_token_id\": null,\n","    \"exponential_decay_length_penalty\": null,\n","    \"finetuning_task\": null,\n","    \"forced_bos_token_id\": null,\n","    \"forced_eos_token_id\": null,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.1,\n","    \"hidden_size\": 768,\n","    \"id2label\": {\n","      \"0\": \"LABEL_0\",\n","      \"1\": \"LABEL_1\"\n","    },\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 3072,\n","    \"is_decoder\": false,\n","    \"is_encoder_decoder\": false,\n","    \"label2id\": {\n","      \"LABEL_0\": 0,\n","      \"LABEL_1\": 1\n","    },\n","    \"layer_norm_eps\": 1e-12,\n","    \"length_penalty\": 1.0,\n","    \"max_length\": 20,\n","    \"max_position_embeddings\": 512,\n","    \"min_length\": 0,\n","    \"model_type\": \"bert\",\n","    \"no_repeat_ngram_size\": 0,\n","    \"num_attention_heads\": 12,\n","    \"num_beam_groups\": 1,\n","    \"num_beams\": 1,\n","    \"num_hidden_layers\": 12,\n","    \"num_return_sequences\": 1,\n","    \"output_attentions\": false,\n","    \"output_hidden_states\": false,\n","    \"output_scores\": false,\n","    \"pad_token_id\": 0,\n","    \"pooler_fc_size\": 768,\n","    \"pooler_num_attention_heads\": 12,\n","    \"pooler_num_fc_layers\": 3,\n","    \"pooler_size_per_head\": 128,\n","    \"pooler_type\": \"first_token_transform\",\n","    \"position_embedding_type\": \"absolute\",\n","    \"prefix\": null,\n","    \"problem_type\": null,\n","    \"pruned_heads\": {},\n","    \"remove_invalid_values\": false,\n","    \"repetition_penalty\": 1.0,\n","    \"return_dict\": true,\n","    \"return_dict_in_generate\": false,\n","    \"sep_token_id\": null,\n","    \"suppress_tokens\": null,\n","    \"task_specific_params\": null,\n","    \"temperature\": 1.0,\n","    \"tf_legacy_loss\": false,\n","    \"tie_encoder_decoder\": false,\n","    \"tie_word_embeddings\": true,\n","    \"tokenizer_class\": null,\n","    \"top_k\": 50,\n","    \"top_p\": 1.0,\n","    \"torch_dtype\": null,\n","    \"torchscript\": false,\n","    \"type_vocab_size\": 2,\n","    \"typical_p\": 1.0,\n","    \"use_bfloat16\": false,\n","    \"use_cache\": true,\n","    \"vocab_size\": 119547\n","  },\n","  \"eos_token_id\": 102,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 75,\n","  \"min_length\": 25,\n","  \"model_type\": \"encoder-decoder\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 3,\n","  \"pad_token_id\": 0,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.41.1\",\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/model.safetensors\n","Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 101,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 102,\n","  \"max_length\": 75,\n","  \"min_length\": 25,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 3,\n","  \"pad_token_id\": 0\n","}\n","\n","Generate config GenerationConfig {\n","  \"pad_token_id\": 0\n","}\n","\n","All model checkpoint weights were used when initializing EncoderDecoderModel.\n","\n","All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n","loading configuration file /content/drive/MyDrive/jax and flax/bert-base-multilingual-cased_to_bert-base-multilingual-cased/best_model_fold_1/generation_config.json\n","Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 101,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 102,\n","  \"max_length\": 75,\n","  \"min_length\": 25,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 3,\n","  \"pad_token_id\": 0\n","}\n","\n"]}],"source":["src_tokenizer, tgt_tokenizer, model, device = load_model(cfg, checkpoint_name=\"best_model_fold_1\")"]},{"cell_type":"code","execution_count":26,"id":"qWHg9XUiDvzz","metadata":{"id":"qWHg9XUiDvzz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717579574758,"user_tz":-420,"elapsed":25283,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"}},"outputId":"94a9021c-c63f-4a09-c8b7-0d5f620f7b84"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:25<00:00,  6.26s/it]\n"]}],"source":["pred_texts = inference_bath(src_texts, src_tokenizer, tgt_tokenizer, model, device, beam_size=2)"]},{"cell_type":"code","execution_count":27,"id":"Nu-fXb7pETXi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1717579574759,"user":{"displayName":"Nghia Dang","userId":"00292144726530360468"},"user_tz":-420},"id":"Nu-fXb7pETXi","outputId":"cbbe2c5d-0c99-4b0d-de7a-f2bdb542d8c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BLEU = 2.13 22.1/5.0/0.8/0.2 (BP = 1.000 ratio = 1.258 hyp_len = 2503 ref_len = 1990)"]},"metadata":{},"execution_count":27}],"source":["sacrebleu.corpus_bleu(pred_texts, [tgt_texts])"]},{"cell_type":"code","source":[],"metadata":{"id":"Sl_VNnEnP0Vt"},"id":"Sl_VNnEnP0Vt","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuClass":"premium","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ba6f11e15c1f4ef6a4d3b7f9d1493269":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_615f830d34f6403c9ee8e1afbf5305db","IPY_MODEL_75000eefc36040eaaa64c0bbff82063f","IPY_MODEL_1f0e6bc0e8ca4009a3a6d637abc2077b"],"layout":"IPY_MODEL_b81eb5187fdb45bdb6825cc879733134"}},"615f830d34f6403c9ee8e1afbf5305db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb2257f7cf5b4451b1c67534ffa14a1e","placeholder":"​","style":"IPY_MODEL_f8d2ce8b3ef2457b911605266efa19db","value":"tokenizer_config.json: 100%"}},"75000eefc36040eaaa64c0bbff82063f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc5e09a996e94951b24e6f5dcf6680b9","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e84057dca0354602971def906ebe41af","value":49}},"1f0e6bc0e8ca4009a3a6d637abc2077b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b6bb6de5f824615a98133136658961f","placeholder":"​","style":"IPY_MODEL_839057c168a048988417cffd439d8064","value":" 49.0/49.0 [00:00&lt;00:00, 3.09kB/s]"}},"b81eb5187fdb45bdb6825cc879733134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb2257f7cf5b4451b1c67534ffa14a1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d2ce8b3ef2457b911605266efa19db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc5e09a996e94951b24e6f5dcf6680b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e84057dca0354602971def906ebe41af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b6bb6de5f824615a98133136658961f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"839057c168a048988417cffd439d8064":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7abe32d24621486ba87d36a5e547927a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da73ec5b0522420099bf54b015203439","IPY_MODEL_8a5077e7a6f64bb58b45f9665ac33cc8","IPY_MODEL_455559d97c354479ba0bc0e4ffb173c4"],"layout":"IPY_MODEL_8e21f2375941428a9842242ece2c346c"}},"da73ec5b0522420099bf54b015203439":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d717966b0bb40939e89f75f6d72b560","placeholder":"​","style":"IPY_MODEL_5c71ad3512e348d7a193ba053ab689cf","value":"config.json: 100%"}},"8a5077e7a6f64bb58b45f9665ac33cc8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06609e39eec04943b04e53502df25e37","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0dcbe87944ac4ef38bd1f4960e2ced13","value":625}},"455559d97c354479ba0bc0e4ffb173c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3462d478b6c04dfc841674f842c05c3c","placeholder":"​","style":"IPY_MODEL_13b8531d354a4873a2de3d2d8671c994","value":" 625/625 [00:00&lt;00:00, 40.6kB/s]"}},"8e21f2375941428a9842242ece2c346c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d717966b0bb40939e89f75f6d72b560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c71ad3512e348d7a193ba053ab689cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06609e39eec04943b04e53502df25e37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dcbe87944ac4ef38bd1f4960e2ced13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3462d478b6c04dfc841674f842c05c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13b8531d354a4873a2de3d2d8671c994":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afa8c58d86854543863e5752e1f33da5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70b15cfb6df24386a75dd0c6851a7f24","IPY_MODEL_7af8439a550244f4b7bd389164b7e048","IPY_MODEL_245b13f9bc79406d8154c34864f9874d"],"layout":"IPY_MODEL_81482252379943629314b423364d0d5e"}},"70b15cfb6df24386a75dd0c6851a7f24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e67b523c8ec4e45b1b627484eaf934c","placeholder":"​","style":"IPY_MODEL_e76da80ecb2b4e948acb307da1e3cc6e","value":"vocab.txt: 100%"}},"7af8439a550244f4b7bd389164b7e048":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3ae61216f964ad9ac5bc46a58d178ca","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa6472edc59243cb8be38802bc75f533","value":995526}},"245b13f9bc79406d8154c34864f9874d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ca9719ac46648fa9d7a877aa229f7be","placeholder":"​","style":"IPY_MODEL_c9332171eac64d46ac315993df962343","value":" 996k/996k [00:00&lt;00:00, 9.06MB/s]"}},"81482252379943629314b423364d0d5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e67b523c8ec4e45b1b627484eaf934c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76da80ecb2b4e948acb307da1e3cc6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3ae61216f964ad9ac5bc46a58d178ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa6472edc59243cb8be38802bc75f533":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ca9719ac46648fa9d7a877aa229f7be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9332171eac64d46ac315993df962343":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f287d907490743e6a29f422b75073e02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4f05dd8c3874057a8310956f8d8f545","IPY_MODEL_2ad1353dc75f42bba00755aa0f5d5eb6","IPY_MODEL_f1c4e2c90bee4b67bda6e2e1f0e55e1f"],"layout":"IPY_MODEL_d6d8218933154b0eae846f2f746b487c"}},"b4f05dd8c3874057a8310956f8d8f545":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6be7767ed3f4f45a5ef699a558cf960","placeholder":"​","style":"IPY_MODEL_6bf78d90f9b0413ba89f4d695a81daba","value":"tokenizer.json: 100%"}},"2ad1353dc75f42bba00755aa0f5d5eb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75770492171843418a0699aeecd7efef","max":1961828,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69d1b1811cd54ccd95ae08ba88e18811","value":1961828}},"f1c4e2c90bee4b67bda6e2e1f0e55e1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74f5d03fc95b49669556406d2ab789c9","placeholder":"​","style":"IPY_MODEL_c04a3dbf16784e95a1765f8941c97bd5","value":" 1.96M/1.96M [00:00&lt;00:00, 14.5MB/s]"}},"d6d8218933154b0eae846f2f746b487c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6be7767ed3f4f45a5ef699a558cf960":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf78d90f9b0413ba89f4d695a81daba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75770492171843418a0699aeecd7efef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d1b1811cd54ccd95ae08ba88e18811":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74f5d03fc95b49669556406d2ab789c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c04a3dbf16784e95a1765f8941c97bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1add8f686d74f3ea078aebaba14e489":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fbbd16f78bc41a1bfb81a94ab031b7b","IPY_MODEL_9888a0f6df6f46d9b6eb47607a829db6","IPY_MODEL_ebe8b643d7b14171958576832f7c4a9f"],"layout":"IPY_MODEL_9e3aff07e210471e8f367f1739db14b5"}},"9fbbd16f78bc41a1bfb81a94ab031b7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7ab194bdd6348a9b1d60e31d18e2f02","placeholder":"​","style":"IPY_MODEL_5c1361884b944ce8860dff80ec39e6af","value":"model.safetensors: 100%"}},"9888a0f6df6f46d9b6eb47607a829db6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3a023bf84c45998986ff1bf788b0c0","max":714290682,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4ce2e016e5749d6a522538b7532f6d6","value":714290682}},"ebe8b643d7b14171958576832f7c4a9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52ef2bd868274d59b8a0621010686fba","placeholder":"​","style":"IPY_MODEL_0977361ef1d54d7194b0874dae08e5dd","value":" 714M/714M [00:04&lt;00:00, 175MB/s]"}},"9e3aff07e210471e8f367f1739db14b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ab194bdd6348a9b1d60e31d18e2f02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c1361884b944ce8860dff80ec39e6af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e3a023bf84c45998986ff1bf788b0c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4ce2e016e5749d6a522538b7532f6d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52ef2bd868274d59b8a0621010686fba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0977361ef1d54d7194b0874dae08e5dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9478cd4f615b466b93dcd19fae703674":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da9e1c858f304908838c60af50158c50","IPY_MODEL_6e59a3038bf4440daa757578b4e6b264","IPY_MODEL_306d8bf6046847e8878d16cf3858382f"],"layout":"IPY_MODEL_720cf32032fd4b358097cbd287278803"}},"da9e1c858f304908838c60af50158c50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc7a9b627aa641cbab897fda1a079a90","placeholder":"​","style":"IPY_MODEL_120458580ab74e2b8784875ade240e2e","value":"Downloading builder script: "}},"6e59a3038bf4440daa757578b4e6b264":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3cc8821c7f64fb181d91202fdcfe655","max":2849,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf10cf6dce264a109b58be433f4da8ce","value":2849}},"306d8bf6046847e8878d16cf3858382f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2560a18aa0274803b73a0012e0744bc7","placeholder":"​","style":"IPY_MODEL_b2a286e33a684eb3b4dd4f746bb54c3f","value":" 7.65k/? [00:00&lt;00:00, 361kB/s]"}},"720cf32032fd4b358097cbd287278803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7a9b627aa641cbab897fda1a079a90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"120458580ab74e2b8784875ade240e2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3cc8821c7f64fb181d91202fdcfe655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf10cf6dce264a109b58be433f4da8ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2560a18aa0274803b73a0012e0744bc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2a286e33a684eb3b4dd4f746bb54c3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}